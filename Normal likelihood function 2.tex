\documentclass[,oneside]{article}
\usepackage{amsmath}
\usepackage{setspace
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{graphicx}
\numberwithin{equation}{section}

\author{Lewis McConkey}
\date{\today}
\title{ \LARGE{Normal Likelihood Function}}
     

          
\begin{document}
\maketitle
\section{Key Definitions}
The Likelihood function of the parameter $\theta$ is the probability of the observed data for given values of $\theta$. Assuming that the data are independent and identically distributed  the likelihood function is:
\begin{center}
$L(\theta|x_1, x_2,..., x_n)= \prod\limits_{ i = 1}^{n} f(x_i|\theta)$
\end{center}
Where $f(x_i|\theta)$ is the probability density function of the data.\\ \\
The Maximum likelihood estimate (MLE) is the value of $\theta$ that maximises $L(\theta|x_1, x_2,..., x_n)$ and is denoted $\hat{\theta}.$ The maximum likelihood estimator of $\theta$ based on random variables $X_1,X_2,...,X_n$ will be denoted $\hat{\theta}(X)$\\ \\

The log-likelihood function is the natural logarithm of the likelihood function denoted:
\begin{center}
$l(\theta|x_1, ..., x_n)= log\left\{L(\theta|x_1,..., x_n)\right\}=log\left \{\prod\limits_{ i = 1}^{n} f(x_i|\theta)\right \}=\sum\limits_{i=1}^{n} log\{f(x_i|\theta)\}$
\end{center}
We often use this instead of the likelihood function as it's often easier to compute things using sum notation instead of products which will become clearer further on in this document.

\section{Normal Distribution}
Since the probability mass function of a normal distribution is:
\begin{center}
$P(X=x)=\frac{1}{\sqrt{2\pi\sigma^2}}exp\left\{-\frac{(x_i-\mu)^2}{2\sigma^2}\right\}$
\end{center}
The Likelihood function is:
\begin{align*}
L(\mu|x_1, x_2,..., x_n)= \prod\limits_{ i = 1}^{n} f(x_i|\mu)&=\prod\limits_{ i = 1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}}exp\left\{-\frac{(x_i-\mu)^2}{2\sigma^2}\right\} \\
&=(2\pi\sigma^2)^{-n/2} exp\left\{-\sum_{i=1}^{n}-\frac{(x_i-\mu)^2}{2\sigma^2}\right\}
\end{align}
Using the definition provided in section 1 for the log-likelihood function we can compute it for the normal distribution:
\begin{center}
$l(\mu)=-\frac{n}{2}log(2\pi\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2$
\end{center}
Now to find the MLE we should first differentiate either the likelihood function or the log likelihood function (these will both be the same since log is a monotone increasing function). Here we will see why the log likelihood is so important since it's much easier in general to differentiate it:
\begin{align*}
l'(\mu)=-\frac{1}{2\sigma^2}\sum_{i=1}^{n}(-2)(x_i-\mu)
\end{align}
Now if we set this derivative equal to 0 then we could potentially get the MLE:
\begin{align*}
\frac{1}{\sigma^2}\sum\limits_{i=1}^{n}x_i-n\hat{\mu}= 0
\end{align}
Hence: 
\begin{align*}
\hat{\mu}=\bar{x}
\end{align}
Finally to confirm this is actually the MLE we need to show that this is a maximum:
\begin{align*}
l''(\mu)=-n < 0
\end{align}
Therefore this is the MLE for a normal distribution.


\end{document}